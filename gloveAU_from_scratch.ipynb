{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gloveAU_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsatxx0LFFVR57yfEeBRC2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtran14/AUglove/blob/main/gloveAU_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzvYA3fQ3YC2",
        "outputId": "86747e04-f0d3-485d-ae86-7951c3a30281"
      },
      "source": [
        "!pip install scikit-learn==0.21.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.21.0\n",
            "  Downloading scikit_learn-0.21.0-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.0) (1.19.5)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DivxPvf5mpZ",
        "outputId": "fadac7bb-badf-469e-aedf-1ca8c7518cd6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, os\n",
        "\n",
        "#create a random centroid file (to test the code)\n",
        "x = np.random.rand(1000,17)\n",
        "print(x.shape)\n",
        "pd.DataFrame(x).to_csv('centroids.csv', header=None, index=False)\n",
        "\n",
        "#create a path containing random samples of openface AU data (to test the code)\n",
        "idx = 0\n",
        "output_path = 'openfaceAU/'\n",
        "while(idx < 5000):\n",
        "  seq_len = np.random.randint(5, 20)\n",
        "  current_data = np.random.rand(seq_len, 17)\n",
        "  output_file = os.path.join(output_path, str(idx)+'.csv')\n",
        "  pd.DataFrame(current_data).to_csv(output_file, header=None, index=False)\n",
        "  idx += 1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n4kr2ar4J-c"
      },
      "source": [
        "#if we don't have the AU centroids, use km-cuda as follow\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from libKMCUDA import kmeans_cuda\n",
        "\n",
        "VOCAB_SIZE = 1000\n",
        "data_path = \"all_aus.csv\" #this file contains 5.3M frames x 17AUs\n",
        "\n",
        "\n",
        "data = pd.read_csv(data_path, header=None).values\n",
        "data = np.array(data,dtype='float32')\n",
        "\n",
        "centroids, assignments = kmeans_cuda(data, VOCAB_SIZE, verbosity=0, seed=0)\n",
        "    \n",
        "pd.DataFrame(centroids).to_csv(\"centroids.csv\",header=None,index=False)\n",
        "pd.DataFrame(assignments).to_csv(\"assignments.csv\",header=None,index=False)\n",
        "from collections import Counter\n",
        "cnt_dict = Counter(assignments)\n",
        "output = []\n",
        "for key in cnt_dict.keys():\n",
        "  output.append([key, cnt_dict[key]])\n",
        "pd.DataFrame(output).to_csv('cluster_counter.csv', header=None, index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAbsmtlu5Woh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "centroids = pd.read_csv('centroids.csv', header=None).values\n",
        "openfaceAU_all_file_path = 'openfaceAU/'\n",
        "\n",
        "K = centroids.shape[1]\n",
        "kmeans = KMeans(n_clusters=K)\n",
        "kmeans.cluster_centers_ = centroids\n",
        "\n",
        "output = []\n",
        "for file in os.listdir(openfaceAU_all_file_path):\n",
        "  current_file_path = os.path.join(openfaceAU_all_file_path, file)\n",
        "  current_data = pd.read_csv(current_file_path, header=None).values\n",
        "  current_seq = kmeans.predict(current_data)\n",
        "  current_seq_str = ' '.join(np.array(current_seq, dtype=str))\n",
        "\n",
        "  current_dist = []\n",
        "  for i in range(current_data.shape[0]):\n",
        "    d = np.linalg.norm(current_data[i]-centroids[current_seq[i]])\n",
        "    current_dist.append(d)\n",
        "  current_ds_str = ' '.join(np.array(current_dist, dtype=str))\n",
        "  output.append(current_seq_str+ ' '+ current_ds_str)\n",
        "\n",
        "output_str = '\\n'.join(output)\n",
        "text_file = open(\"cluster_label_data.txt\", \"w\")\n",
        "text_file.write(output_str)\n",
        "text_file.close()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvS-qMQ572Su"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def read_corpus(filepath):\n",
        "    START_TOKEN = '<START>'\n",
        "    END_TOKEN = '<END>'\n",
        "    K = 5000\n",
        "    distance_threshold = 1.75\n",
        "    \n",
        "    freq_threshold = 500\n",
        "    data_freq = pd.read_csv(\"cluster_counter.csv\", header=None).values\n",
        "    unk_clusters = []\n",
        "    for i in range(data_freq.shape[0]):\n",
        "        if(data_freq[i][1] <= freq_threshold):\n",
        "            unk_clusters.append(data_freq[i][0])\n",
        "            \n",
        "    with open(filepath) as f:\n",
        "        content = f.readlines()\n",
        "    content = [x.strip() for x in content] \n",
        "    \n",
        "    output = []\n",
        "    for row in content:\n",
        "        row_tokens = [START_TOKEN]\n",
        "        row = row.replace('\\n','')\n",
        "        tokens = row.split()\n",
        "        assert len(tokens) % 2 == 0\n",
        "        text_part = tokens[0:len(tokens)//2]\n",
        "        distances_part = tokens[len(tokens)//2:len(tokens)]\n",
        "        current_str = ''\n",
        "        for i in range(len(text_part)):\n",
        "            current_token = text_part[i]\n",
        "            current_distance = float(distances_part[i])\n",
        "            if(current_distance <= distance_threshold):\n",
        "                try:\n",
        "                  current_cluster_in_int = int(current_token)\n",
        "                except:\n",
        "                  current_cluster_in_int = int(current_token[1:])\n",
        "                if(current_cluster_in_int not in unk_clusters):\n",
        "                    row_tokens.append(current_token)\n",
        "                else:\n",
        "                    row_tokens.append(\"<unk>\")\n",
        "            else:\n",
        "                row_tokens.append(\"<unk>\")\n",
        "        row_tokens.append(END_TOKEN)\n",
        "        output.append(row_tokens)\n",
        "        \n",
        "    return output\n",
        "\n",
        "def distinct_words(corpus):\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "    flattened_list = [word for article in corpus for word in article]\n",
        "    unique_words_set = set(flattened_list) # keep unique words only\n",
        "    unique_word_list = [word for word in unique_words_set] # convert set back to a list, then sort it\n",
        "    corpus_words = sorted(unique_word_list) # list of sorted, unique words \n",
        "    num_corpus_words = len(corpus_words)\n",
        "    return corpus_words, num_corpus_words\n",
        "\n",
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    M = None\n",
        "    word2Ind = {}\n",
        "    for i in range(num_words):\n",
        "        word2Ind[words[i]] = i\n",
        "\n",
        "    M = np.zeros((num_words, num_words))\n",
        "    \n",
        "    for line in corpus:\n",
        "        for i in range(len(line)):\n",
        "            target = line[i]\n",
        "            target_index = word2Ind[target]\n",
        "            \n",
        "            left = max(i - window_size, 0)\n",
        "            right = min(i + window_size, len(line) - 1)\n",
        "\n",
        "            for j in range(left, i):\n",
        "                window_word = line[j]\n",
        "                M[target_index][word2Ind[window_word]] += 1\n",
        "                M[word2Ind[window_word]][target_index] += 1\n",
        "    return M, word2Ind\n",
        "\n",
        "corpus = read_corpus(\"cluster_label_data.txt\")\n",
        "M, word2Ind = compute_co_occurrence_matrix(corpus, window_size=10)\n",
        "        \n",
        "pd.DataFrame(M).to_csv(\"occurence_matrix_1000.csv\", header=None, index=False)\n",
        "output_w2i = []\n",
        "for i in word2Ind.keys():\n",
        "    output_w2i.append([i, word2Ind[i]])\n",
        "pd.DataFrame(output_w2i).to_csv(\"word2ind_1000.csv\", header=None, index=False)  \n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPf2r7GuCppB",
        "outputId": "e736c4d7-7a88-4f74-bc10-3798c926b11b"
      },
      "source": [
        "!pip install mittens"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mittens\n",
            "  Downloading mittens-0.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.19.5)\n",
            "Installing collected packages: mittens\n",
            "Successfully installed mittens-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KB-Q4k177Di",
        "outputId": "9405d885-1330-48e1-f562-c57e73f5de9f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mittens import GloVe\n",
        "\n",
        "data_path = \"occurence_matrix_1000.csv\"\n",
        "data = pd.read_csv(data_path, header=None).values\n",
        "emb_dimension = 100\n",
        "n_iter = 1000 #for testing purpose, increase until converge\n",
        "\n",
        "glove_model = GloVe(n=emb_dimension, max_iter=n_iter) \n",
        "embeddings = glove_model.fit(data)\n",
        "\n",
        "pd.DataFrame(embeddings).to_csv(\"glove_embeddings.csv\", header=None, index=False)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1766: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "Iteration 1000: loss: 602.243896484375"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}